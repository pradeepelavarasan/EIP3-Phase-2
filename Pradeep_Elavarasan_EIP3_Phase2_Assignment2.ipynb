{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pradeep_Elavarasan_EIP3_Phase2_Assignment2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradeepelavarasan/EIP3-Phase-2/blob/master/Pradeep_Elavarasan_EIP3_Phase2_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJb68dqmf55X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-2Bi_o1snL0",
        "colab_type": "code",
        "outputId": "2f2a4d00-8cf6-4812-ffef-4be094626e2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAPPeW9TfsY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5f85T-jjg-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh9HJ0ygj06p",
        "colab_type": "code",
        "outputId": "2d44b02f-b8a5-452d-a7ec-78730c7236c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \" + str(n_chars))\n",
        "print(\"Total Vocab: \" + str(n_vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters: 137953\n",
            "Total Vocab: 38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RXayBn-j12J",
        "colab_type": "code",
        "outputId": "856b18a3-ed1b-4dc2-cd83-fbfae9378d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \" +str(n_patterns))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns: 137853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44P5j0k9mxEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqDy3sljiNMD",
        "colab_type": "code",
        "outputId": "4a94414c-7eba-4112-da3f-b675ed970734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256,dropout=0.1,input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0727 01:14:01.032143 140248496220032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0727 01:14:01.076628 140248496220032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0727 01:14:01.085081 140248496220032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0727 01:14:01.296800 140248496220032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0727 01:14:01.308469 140248496220032 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0727 01:14:01.809674 140248496220032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0727 01:14:01.834066 140248496220032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYByCkkLj_ZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBQokpk5kIPt",
        "colab_type": "code",
        "outputId": "c9833a5d-c8e5-487b-b463-0ecaa279eb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=20, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0726 16:57:25.372871 140000892417920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "137853/137853 [==============================] - 168s 1ms/step - loss: 2.8739\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.87387, saving model to weights-improvement-01-2.8739.hdf5\n",
            "Epoch 2/20\n",
            "137853/137853 [==============================] - 164s 1ms/step - loss: 2.6147\n",
            "\n",
            "Epoch 00002: loss improved from 2.87387 to 2.61466, saving model to weights-improvement-02-2.6147.hdf5\n",
            "Epoch 3/20\n",
            "137853/137853 [==============================] - 162s 1ms/step - loss: 2.4484\n",
            "\n",
            "Epoch 00003: loss improved from 2.61466 to 2.44842, saving model to weights-improvement-03-2.4484.hdf5\n",
            "Epoch 4/20\n",
            "137853/137853 [==============================] - 161s 1ms/step - loss: 2.2922\n",
            "\n",
            "Epoch 00004: loss improved from 2.44842 to 2.29223, saving model to weights-improvement-04-2.2922.hdf5\n",
            "Epoch 5/20\n",
            "137853/137853 [==============================] - 161s 1ms/step - loss: 2.2313\n",
            "\n",
            "Epoch 00005: loss improved from 2.29223 to 2.23128, saving model to weights-improvement-05-2.2313.hdf5\n",
            "Epoch 6/20\n",
            "137853/137853 [==============================] - 161s 1ms/step - loss: 2.1713\n",
            "\n",
            "Epoch 00006: loss improved from 2.23128 to 2.17134, saving model to weights-improvement-06-2.1713.hdf5\n",
            "Epoch 7/20\n",
            "137853/137853 [==============================] - 160s 1ms/step - loss: 2.0856\n",
            "\n",
            "Epoch 00007: loss improved from 2.17134 to 2.08557, saving model to weights-improvement-07-2.0856.hdf5\n",
            "Epoch 8/20\n",
            "137853/137853 [==============================] - 160s 1ms/step - loss: 2.0092\n",
            "\n",
            "Epoch 00008: loss improved from 2.08557 to 2.00917, saving model to weights-improvement-08-2.0092.hdf5\n",
            "Epoch 9/20\n",
            "137853/137853 [==============================] - 160s 1ms/step - loss: 1.9394\n",
            "\n",
            "Epoch 00009: loss improved from 2.00917 to 1.93938, saving model to weights-improvement-09-1.9394.hdf5\n",
            "Epoch 10/20\n",
            "137853/137853 [==============================] - 160s 1ms/step - loss: 1.8790\n",
            "\n",
            "Epoch 00010: loss improved from 1.93938 to 1.87898, saving model to weights-improvement-10-1.8790.hdf5\n",
            "Epoch 11/20\n",
            "137853/137853 [==============================] - 161s 1ms/step - loss: 1.8228\n",
            "\n",
            "Epoch 00011: loss improved from 1.87898 to 1.82283, saving model to weights-improvement-11-1.8228.hdf5\n",
            "Epoch 12/20\n",
            "137853/137853 [==============================] - 162s 1ms/step - loss: 1.7809\n",
            "\n",
            "Epoch 00012: loss improved from 1.82283 to 1.78087, saving model to weights-improvement-12-1.7809.hdf5\n",
            "Epoch 13/20\n",
            "137853/137853 [==============================] - 161s 1ms/step - loss: 1.7375\n",
            "\n",
            "Epoch 00013: loss improved from 1.78087 to 1.73750, saving model to weights-improvement-13-1.7375.hdf5\n",
            "Epoch 14/20\n",
            "137853/137853 [==============================] - 163s 1ms/step - loss: 1.7018\n",
            "\n",
            "Epoch 00014: loss improved from 1.73750 to 1.70176, saving model to weights-improvement-14-1.7018.hdf5\n",
            "Epoch 15/20\n",
            "137853/137853 [==============================] - 164s 1ms/step - loss: 1.6676\n",
            "\n",
            "Epoch 00015: loss improved from 1.70176 to 1.66765, saving model to weights-improvement-15-1.6676.hdf5\n",
            "Epoch 16/20\n",
            "137853/137853 [==============================] - 164s 1ms/step - loss: 1.6339\n",
            "\n",
            "Epoch 00016: loss improved from 1.66765 to 1.63385, saving model to weights-improvement-16-1.6339.hdf5\n",
            "Epoch 17/20\n",
            "137853/137853 [==============================] - 164s 1ms/step - loss: 1.6028\n",
            "\n",
            "Epoch 00017: loss improved from 1.63385 to 1.60285, saving model to weights-improvement-17-1.6028.hdf5\n",
            "Epoch 18/20\n",
            "137853/137853 [==============================] - 164s 1ms/step - loss: 1.5739\n",
            "\n",
            "Epoch 00018: loss improved from 1.60285 to 1.57388, saving model to weights-improvement-18-1.5739.hdf5\n",
            "Epoch 19/20\n",
            "137853/137853 [==============================] - 164s 1ms/step - loss: 1.5438\n",
            "\n",
            "Epoch 00019: loss improved from 1.57388 to 1.54384, saving model to weights-improvement-19-1.5438.hdf5\n",
            "Epoch 20/20\n",
            "137853/137853 [==============================] - 164s 1ms/step - loss: 1.5188\n",
            "\n",
            "Epoch 00020: loss improved from 1.54384 to 1.51879, saving model to weights-improvement-20-1.5188.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5463f595c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rWCcktjsBna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"/content/gdrive/My Drive/Assignment_6/Training_Backup/Assignment6_V2_1-20.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dwqeokAcDM3",
        "colab_type": "code",
        "outputId": "2dcd7ed3-efcb-4cc4-b371-3853f4ee2aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.load_weights(\"/content/gdrive/My Drive/Assignment_6/Training_Backup/Assignment6_V2_1-20.hdf5\")\n",
        "model.fit(X, y, epochs=50, batch_size=256, callbacks=callbacks_list, initial_epoch=20)\n",
        "model.save_weights(\"/content/gdrive/My Drive/Assignment_6/Training_Backup/Assignment6_V2_21-50.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/50\n",
            "137853/137853 [==============================] - 164s 1ms/step - loss: 1.4922\n",
            "\n",
            "Epoch 00021: loss improved from 1.51879 to 1.49220, saving model to weights-improvement-21-1.4922.hdf5\n",
            "Epoch 22/50\n",
            "137853/137853 [==============================] - 163s 1ms/step - loss: 1.4676\n",
            "\n",
            "Epoch 00022: loss improved from 1.49220 to 1.46758, saving model to weights-improvement-22-1.4676.hdf5\n",
            "Epoch 23/50\n",
            "137853/137853 [==============================] - 164s 1ms/step - loss: 1.4455\n",
            "\n",
            "Epoch 00023: loss improved from 1.46758 to 1.44553, saving model to weights-improvement-23-1.4455.hdf5\n",
            "Epoch 24/50\n",
            "137853/137853 [==============================] - 163s 1ms/step - loss: 1.4227\n",
            "\n",
            "Epoch 00024: loss improved from 1.44553 to 1.42273, saving model to weights-improvement-24-1.4227.hdf5\n",
            "Epoch 25/50\n",
            "137853/137853 [==============================] - 164s 1ms/step - loss: 1.4008\n",
            "\n",
            "Epoch 00025: loss improved from 1.42273 to 1.40082, saving model to weights-improvement-25-1.4008.hdf5\n",
            "Epoch 26/50\n",
            "137853/137853 [==============================] - 163s 1ms/step - loss: 1.3774\n",
            "\n",
            "Epoch 00026: loss improved from 1.40082 to 1.37743, saving model to weights-improvement-26-1.3774.hdf5\n",
            "Epoch 27/50\n",
            "137853/137853 [==============================] - 164s 1ms/step - loss: 1.3596\n",
            "\n",
            "Epoch 00027: loss improved from 1.37743 to 1.35957, saving model to weights-improvement-27-1.3596.hdf5\n",
            "Epoch 28/50\n",
            "137853/137853 [==============================] - 163s 1ms/step - loss: 1.3356\n",
            "\n",
            "Epoch 00028: loss improved from 1.35957 to 1.33561, saving model to weights-improvement-28-1.3356.hdf5\n",
            "Epoch 29/50\n",
            "137853/137853 [==============================] - 158s 1ms/step - loss: 1.3183\n",
            "\n",
            "Epoch 00029: loss improved from 1.33561 to 1.31832, saving model to weights-improvement-29-1.3183.hdf5\n",
            "Epoch 30/50\n",
            "137853/137853 [==============================] - 157s 1ms/step - loss: 1.2997\n",
            "\n",
            "Epoch 00030: loss improved from 1.31832 to 1.29968, saving model to weights-improvement-30-1.2997.hdf5\n",
            "Epoch 31/50\n",
            "137853/137853 [==============================] - 159s 1ms/step - loss: 1.2807\n",
            "\n",
            "Epoch 00031: loss improved from 1.29968 to 1.28065, saving model to weights-improvement-31-1.2807.hdf5\n",
            "Epoch 32/50\n",
            "137853/137853 [==============================] - 157s 1ms/step - loss: 1.2643\n",
            "\n",
            "Epoch 00032: loss improved from 1.28065 to 1.26428, saving model to weights-improvement-32-1.2643.hdf5\n",
            "Epoch 33/50\n",
            "137853/137853 [==============================] - 160s 1ms/step - loss: 1.2447\n",
            "\n",
            "Epoch 00033: loss improved from 1.26428 to 1.24466, saving model to weights-improvement-33-1.2447.hdf5\n",
            "Epoch 34/50\n",
            "137853/137853 [==============================] - 159s 1ms/step - loss: 1.2254\n",
            "\n",
            "Epoch 00034: loss improved from 1.24466 to 1.22535, saving model to weights-improvement-34-1.2254.hdf5\n",
            "Epoch 35/50\n",
            "137853/137853 [==============================] - 159s 1ms/step - loss: 1.2152\n",
            "\n",
            "Epoch 00035: loss improved from 1.22535 to 1.21517, saving model to weights-improvement-35-1.2152.hdf5\n",
            "Epoch 36/50\n",
            "137853/137853 [==============================] - 159s 1ms/step - loss: 1.1921\n",
            "\n",
            "Epoch 00036: loss improved from 1.21517 to 1.19206, saving model to weights-improvement-36-1.1921.hdf5\n",
            "Epoch 37/50\n",
            "137853/137853 [==============================] - 159s 1ms/step - loss: 1.1768\n",
            "\n",
            "Epoch 00037: loss improved from 1.19206 to 1.17683, saving model to weights-improvement-37-1.1768.hdf5\n",
            "Epoch 38/50\n",
            "137853/137853 [==============================] - 158s 1ms/step - loss: 1.1614\n",
            "\n",
            "Epoch 00038: loss improved from 1.17683 to 1.16145, saving model to weights-improvement-38-1.1614.hdf5\n",
            "Epoch 39/50\n",
            "137853/137853 [==============================] - 158s 1ms/step - loss: 1.1511\n",
            "\n",
            "Epoch 00039: loss improved from 1.16145 to 1.15108, saving model to weights-improvement-39-1.1511.hdf5\n",
            "Epoch 40/50\n",
            "137853/137853 [==============================] - 158s 1ms/step - loss: 1.1337\n",
            "\n",
            "Epoch 00040: loss improved from 1.15108 to 1.13371, saving model to weights-improvement-40-1.1337.hdf5\n",
            "Epoch 41/50\n",
            "137853/137853 [==============================] - 158s 1ms/step - loss: 1.1198\n",
            "\n",
            "Epoch 00041: loss improved from 1.13371 to 1.11976, saving model to weights-improvement-41-1.1198.hdf5\n",
            "Epoch 42/50\n",
            "137853/137853 [==============================] - 158s 1ms/step - loss: 1.1083\n",
            "\n",
            "Epoch 00042: loss improved from 1.11976 to 1.10829, saving model to weights-improvement-42-1.1083.hdf5\n",
            "Epoch 43/50\n",
            "137853/137853 [==============================] - 158s 1ms/step - loss: 1.0962\n",
            "\n",
            "Epoch 00043: loss improved from 1.10829 to 1.09622, saving model to weights-improvement-43-1.0962.hdf5\n",
            "Epoch 44/50\n",
            "137853/137853 [==============================] - 158s 1ms/step - loss: 1.0822\n",
            "\n",
            "Epoch 00044: loss improved from 1.09622 to 1.08220, saving model to weights-improvement-44-1.0822.hdf5\n",
            "Epoch 45/50\n",
            "137853/137853 [==============================] - 157s 1ms/step - loss: 1.0693\n",
            "\n",
            "Epoch 00045: loss improved from 1.08220 to 1.06934, saving model to weights-improvement-45-1.0693.hdf5\n",
            "Epoch 46/50\n",
            "137853/137853 [==============================] - 157s 1ms/step - loss: 1.0593\n",
            "\n",
            "Epoch 00046: loss improved from 1.06934 to 1.05927, saving model to weights-improvement-46-1.0593.hdf5\n",
            "Epoch 47/50\n",
            "137853/137853 [==============================] - 158s 1ms/step - loss: 1.0472\n",
            "\n",
            "Epoch 00047: loss improved from 1.05927 to 1.04723, saving model to weights-improvement-47-1.0472.hdf5\n",
            "Epoch 48/50\n",
            "137853/137853 [==============================] - 158s 1ms/step - loss: 1.0385\n",
            "\n",
            "Epoch 00048: loss improved from 1.04723 to 1.03851, saving model to weights-improvement-48-1.0385.hdf5\n",
            "Epoch 49/50\n",
            "137853/137853 [==============================] - 158s 1ms/step - loss: 1.0262\n",
            "\n",
            "Epoch 00049: loss improved from 1.03851 to 1.02615, saving model to weights-improvement-49-1.0262.hdf5\n",
            "Epoch 50/50\n",
            "137853/137853 [==============================] - 159s 1ms/step - loss: 1.0160\n",
            "\n",
            "Epoch 00050: loss improved from 1.02615 to 1.01597, saving model to weights-improvement-50-1.0160.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5T3k1Zg_j3K",
        "colab_type": "code",
        "outputId": "a567cc3e-4645-4f59-da4e-72fec190c836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.load_weights(\"/content/gdrive/My Drive/Assignment_6/Training_Backup/Assignment6_V2_21-50.hdf5\")\n",
        "model.fit(X, y, epochs=100, batch_size=256, callbacks=callbacks_list, initial_epoch=50)\n",
        "model.save_weights(\"/content/gdrive/My Drive/Assignment_6/Training_Backup/Assignment6_V2_51-100.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 51/100\n",
            "137853/137853 [==============================] - 153s 1ms/step - loss: 1.0083\n",
            "\n",
            "Epoch 00051: loss improved from inf to 1.00829, saving model to weights-improvement-51-1.0083.hdf5\n",
            "Epoch 52/100\n",
            "137853/137853 [==============================] - 151s 1ms/step - loss: 0.9950\n",
            "\n",
            "Epoch 00052: loss improved from 1.00829 to 0.99503, saving model to weights-improvement-52-0.9950.hdf5\n",
            "Epoch 53/100\n",
            "137853/137853 [==============================] - 152s 1ms/step - loss: 0.9850\n",
            "\n",
            "Epoch 00053: loss improved from 0.99503 to 0.98505, saving model to weights-improvement-53-0.9850.hdf5\n",
            "Epoch 54/100\n",
            "137853/137853 [==============================] - 151s 1ms/step - loss: 0.9761\n",
            "\n",
            "Epoch 00054: loss improved from 0.98505 to 0.97612, saving model to weights-improvement-54-0.9761.hdf5\n",
            "Epoch 55/100\n",
            "137853/137853 [==============================] - 150s 1ms/step - loss: 0.9661\n",
            "\n",
            "Epoch 00055: loss improved from 0.97612 to 0.96609, saving model to weights-improvement-55-0.9661.hdf5\n",
            "Epoch 56/100\n",
            "137853/137853 [==============================] - 151s 1ms/step - loss: 0.9561\n",
            "\n",
            "Epoch 00056: loss improved from 0.96609 to 0.95613, saving model to weights-improvement-56-0.9561.hdf5\n",
            "Epoch 57/100\n",
            "137853/137853 [==============================] - 151s 1ms/step - loss: 0.9510\n",
            "\n",
            "Epoch 00057: loss improved from 0.95613 to 0.95103, saving model to weights-improvement-57-0.9510.hdf5\n",
            "Epoch 58/100\n",
            "137853/137853 [==============================] - 150s 1ms/step - loss: 0.9880\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.95103\n",
            "Epoch 59/100\n",
            "137853/137853 [==============================] - 150s 1ms/step - loss: 1.5515\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.95103\n",
            "Epoch 60/100\n",
            "137853/137853 [==============================] - 149s 1ms/step - loss: 1.2268\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.95103\n",
            "Epoch 61/100\n",
            "137853/137853 [==============================] - 147s 1ms/step - loss: 1.1659\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.95103\n",
            "Epoch 62/100\n",
            "137853/137853 [==============================] - 145s 1ms/step - loss: 1.1355\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.95103\n",
            "Epoch 63/100\n",
            "137853/137853 [==============================] - 146s 1ms/step - loss: 1.1088\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.95103\n",
            "Epoch 64/100\n",
            "137853/137853 [==============================] - 146s 1ms/step - loss: 0.9677\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.95103\n",
            "Epoch 65/100\n",
            "137853/137853 [==============================] - 147s 1ms/step - loss: 0.9353\n",
            "\n",
            "Epoch 00065: loss improved from 0.95103 to 0.93530, saving model to weights-improvement-65-0.9353.hdf5\n",
            "Epoch 66/100\n",
            "137853/137853 [==============================] - 147s 1ms/step - loss: 0.9220\n",
            "\n",
            "Epoch 00066: loss improved from 0.93530 to 0.92197, saving model to weights-improvement-66-0.9220.hdf5\n",
            "Epoch 67/100\n",
            "137853/137853 [==============================] - 147s 1ms/step - loss: 0.9118\n",
            "\n",
            "Epoch 00067: loss improved from 0.92197 to 0.91176, saving model to weights-improvement-67-0.9118.hdf5\n",
            "Epoch 68/100\n",
            "137853/137853 [==============================] - 146s 1ms/step - loss: 0.9037\n",
            "\n",
            "Epoch 00068: loss improved from 0.91176 to 0.90375, saving model to weights-improvement-68-0.9037.hdf5\n",
            "Epoch 69/100\n",
            "137853/137853 [==============================] - 146s 1ms/step - loss: 0.8938\n",
            "\n",
            "Epoch 00069: loss improved from 0.90375 to 0.89378, saving model to weights-improvement-69-0.8938.hdf5\n",
            "Epoch 70/100\n",
            "137853/137853 [==============================] - 145s 1ms/step - loss: 0.8869\n",
            "\n",
            "Epoch 00070: loss improved from 0.89378 to 0.88689, saving model to weights-improvement-70-0.8869.hdf5\n",
            "Epoch 71/100\n",
            "137853/137853 [==============================] - 145s 1ms/step - loss: 0.8790\n",
            "\n",
            "Epoch 00071: loss improved from 0.88689 to 0.87898, saving model to weights-improvement-71-0.8790.hdf5\n",
            "Epoch 72/100\n",
            "137853/137853 [==============================] - 144s 1ms/step - loss: 0.8708\n",
            "\n",
            "Epoch 00072: loss improved from 0.87898 to 0.87080, saving model to weights-improvement-72-0.8708.hdf5\n",
            "Epoch 73/100\n",
            "137853/137853 [==============================] - 142s 1ms/step - loss: 0.8671\n",
            "\n",
            "Epoch 00073: loss improved from 0.87080 to 0.86707, saving model to weights-improvement-73-0.8671.hdf5\n",
            "Epoch 74/100\n",
            "137853/137853 [==============================] - 143s 1ms/step - loss: 0.8637\n",
            "\n",
            "Epoch 00074: loss improved from 0.86707 to 0.86370, saving model to weights-improvement-74-0.8637.hdf5\n",
            "Epoch 75/100\n",
            "137853/137853 [==============================] - 142s 1ms/step - loss: 0.8544\n",
            "\n",
            "Epoch 00075: loss improved from 0.86370 to 0.85443, saving model to weights-improvement-75-0.8544.hdf5\n",
            "Epoch 76/100\n",
            "137853/137853 [==============================] - 141s 1ms/step - loss: 0.8529\n",
            "\n",
            "Epoch 00076: loss improved from 0.85443 to 0.85291, saving model to weights-improvement-76-0.8529.hdf5\n",
            "Epoch 77/100\n",
            "137853/137853 [==============================] - 141s 1ms/step - loss: 0.8508\n",
            "\n",
            "Epoch 00077: loss improved from 0.85291 to 0.85076, saving model to weights-improvement-77-0.8508.hdf5\n",
            "Epoch 78/100\n",
            "137853/137853 [==============================] - 141s 1ms/step - loss: 0.8434\n",
            "\n",
            "Epoch 00078: loss improved from 0.85076 to 0.84341, saving model to weights-improvement-78-0.8434.hdf5\n",
            "Epoch 79/100\n",
            "137853/137853 [==============================] - 141s 1ms/step - loss: 0.8390\n",
            "\n",
            "Epoch 00079: loss improved from 0.84341 to 0.83904, saving model to weights-improvement-79-0.8390.hdf5\n",
            "Epoch 80/100\n",
            "137853/137853 [==============================] - 141s 1ms/step - loss: 0.8275\n",
            "\n",
            "Epoch 00080: loss improved from 0.83904 to 0.82754, saving model to weights-improvement-80-0.8275.hdf5\n",
            "Epoch 81/100\n",
            "137853/137853 [==============================] - 141s 1ms/step - loss: 0.8266\n",
            "\n",
            "Epoch 00081: loss improved from 0.82754 to 0.82662, saving model to weights-improvement-81-0.8266.hdf5\n",
            "Epoch 82/100\n",
            "137853/137853 [==============================] - 140s 1ms/step - loss: 0.8187\n",
            "\n",
            "Epoch 00082: loss improved from 0.82662 to 0.81869, saving model to weights-improvement-82-0.8187.hdf5\n",
            "Epoch 83/100\n",
            "137853/137853 [==============================] - 141s 1ms/step - loss: 0.8117\n",
            "\n",
            "Epoch 00083: loss improved from 0.81869 to 0.81167, saving model to weights-improvement-83-0.8117.hdf5\n",
            "Epoch 84/100\n",
            "137853/137853 [==============================] - 140s 1ms/step - loss: 0.8083\n",
            "\n",
            "Epoch 00084: loss improved from 0.81167 to 0.80828, saving model to weights-improvement-84-0.8083.hdf5\n",
            "Epoch 85/100\n",
            "137853/137853 [==============================] - 141s 1ms/step - loss: 0.8041\n",
            "\n",
            "Epoch 00085: loss improved from 0.80828 to 0.80409, saving model to weights-improvement-85-0.8041.hdf5\n",
            "Epoch 86/100\n",
            "137853/137853 [==============================] - 140s 1ms/step - loss: 0.8039\n",
            "\n",
            "Epoch 00086: loss improved from 0.80409 to 0.80389, saving model to weights-improvement-86-0.8039.hdf5\n",
            "Epoch 87/100\n",
            "137853/137853 [==============================] - 141s 1ms/step - loss: 0.8001\n",
            "\n",
            "Epoch 00087: loss improved from 0.80389 to 0.80009, saving model to weights-improvement-87-0.8001.hdf5\n",
            "Epoch 88/100\n",
            "137853/137853 [==============================] - 140s 1ms/step - loss: 0.7914\n",
            "\n",
            "Epoch 00088: loss improved from 0.80009 to 0.79143, saving model to weights-improvement-88-0.7914.hdf5\n",
            "Epoch 89/100\n",
            "137853/137853 [==============================] - 140s 1ms/step - loss: 0.7856\n",
            "\n",
            "Epoch 00089: loss improved from 0.79143 to 0.78558, saving model to weights-improvement-89-0.7856.hdf5\n",
            "Epoch 90/100\n",
            "137853/137853 [==============================] - 140s 1ms/step - loss: 0.7831\n",
            "\n",
            "Epoch 00090: loss improved from 0.78558 to 0.78306, saving model to weights-improvement-90-0.7831.hdf5\n",
            "Epoch 91/100\n",
            "137853/137853 [==============================] - 141s 1ms/step - loss: 0.7796\n",
            "\n",
            "Epoch 00091: loss improved from 0.78306 to 0.77960, saving model to weights-improvement-91-0.7796.hdf5\n",
            "Epoch 92/100\n",
            "137853/137853 [==============================] - 142s 1ms/step - loss: 0.7763\n",
            "\n",
            "Epoch 00092: loss improved from 0.77960 to 0.77632, saving model to weights-improvement-92-0.7763.hdf5\n",
            "Epoch 93/100\n",
            "137853/137853 [==============================] - 142s 1ms/step - loss: 0.7693\n",
            "\n",
            "Epoch 00093: loss improved from 0.77632 to 0.76925, saving model to weights-improvement-93-0.7693.hdf5\n",
            "Epoch 94/100\n",
            "137853/137853 [==============================] - 143s 1ms/step - loss: 0.7756\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.76925\n",
            "Epoch 95/100\n",
            "137853/137853 [==============================] - 144s 1ms/step - loss: 0.7562\n",
            "\n",
            "Epoch 00095: loss improved from 0.76925 to 0.75622, saving model to weights-improvement-95-0.7562.hdf5\n",
            "Epoch 96/100\n",
            "137853/137853 [==============================] - 144s 1ms/step - loss: 0.7602\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.75622\n",
            "Epoch 97/100\n",
            "137853/137853 [==============================] - 144s 1ms/step - loss: 0.7468\n",
            "\n",
            "Epoch 00097: loss improved from 0.75622 to 0.74676, saving model to weights-improvement-97-0.7468.hdf5\n",
            "Epoch 98/100\n",
            "137853/137853 [==============================] - 144s 1ms/step - loss: 0.7448\n",
            "\n",
            "Epoch 00098: loss improved from 0.74676 to 0.74476, saving model to weights-improvement-98-0.7448.hdf5\n",
            "Epoch 99/100\n",
            "137853/137853 [==============================] - 143s 1ms/step - loss: 0.7362\n",
            "\n",
            "Epoch 00099: loss improved from 0.74476 to 0.73620, saving model to weights-improvement-99-0.7362.hdf5\n",
            "Epoch 100/100\n",
            "137853/137853 [==============================] - 143s 1ms/step - loss: 0.7400\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.73620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCQPIWL6kOIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# load the network weights\n",
        "filename = \"/content/gdrive/My Drive/Assignment_6/Training_Backup/Assignment6_V2_51-100.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1QojZMykcul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7lqN5k1khhv",
        "colab_type": "code",
        "outputId": "4c6adc7c-9bf8-4afc-c286-0454c10950db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" n\n",
            "without a cat! its the most curious thing i ever saw in my life!\n",
            "\n",
            "she had not gone much farther be \"\n",
            "onsi tneees  cryth yhat \"itlee pade iachd prtsinesty sneced tn the\n",
            "tuatdi the wher ot sogase whin: anice whoueht thalinyed one lt nnke youe disser \n",
            "tneasw to thar sfruletne lertiosg whe asi hor suteee!the hel frmlad put andie to lt anooated to tie\n",
            "whveeclente! and the welmer alaie to herp tuatdh in a cay dsoledted wo suatd\n",
            "and to ie wayght tueh a curierid toeeing the whe as slde offv hing i\n",
            "dod- i dodttinta  said the hortsw mut lu suetty anl thadh tfar ot euoihid to terpled\n",
            "at i-uhouldt sfrarkuu\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6HFAZeytF5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}